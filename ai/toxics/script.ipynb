{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5904d3f85fd427897f038e1bf718feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2755e2a323c3412da299098c52db702f",
              "IPY_MODEL_ad27065988314aa4ba2059fdc670d003",
              "IPY_MODEL_ead5cfc8f5514484a88e7f8f23859908"
            ],
            "layout": "IPY_MODEL_dff4b07753f54db39701bc8ec1af9bd1"
          }
        },
        "2755e2a323c3412da299098c52db702f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ab7664ef5f3427bb8ab2af9b48c3cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_2ede1563eb6d4c2da2f37a689fcde905",
            "value": "README.md: "
          }
        },
        "ad27065988314aa4ba2059fdc670d003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78dcb198a6964496b83992b262e0b34f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d54f9becc454f37bb4e063445ab636f",
            "value": 1
          }
        },
        "ead5cfc8f5514484a88e7f8f23859908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5933ac52ad0f4a68a79d4b186e80f902",
            "placeholder": "​",
            "style": "IPY_MODEL_7d80ba12b5b24864a2e3d64cd5dfcfb2",
            "value": " 1.76k/? [00:00&lt;00:00, 181kB/s]"
          }
        },
        "dff4b07753f54db39701bc8ec1af9bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab7664ef5f3427bb8ab2af9b48c3cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ede1563eb6d4c2da2f37a689fcde905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78dcb198a6964496b83992b262e0b34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4d54f9becc454f37bb4e063445ab636f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5933ac52ad0f4a68a79d4b186e80f902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d80ba12b5b24864a2e3d64cd5dfcfb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cl6bj6vWWB6-"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"datasets>=2.19.0\" \"tokenizers>=0.15.2\" \"torch>=2.2\" \"tqdm\" \"scikit-learn\" \"onnx>=1.16\" \"onnxruntime>=1.18\" \"fastapi\" \"uvicorn[standard]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, math, json, gc, sys\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, IterableDataset, Dataset as HFDataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import NFKC, Lowercase, Sequence\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc80wf6IW_2J",
        "outputId": "e2f63081-0967-419a-ddc9-a86fbedf33d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"hf_path\": \"tarudesu/VOZ-HSD\",   # dataset\n",
        "    \"streaming\": True,               # dùng streaming để không tải toàn bộ\n",
        "    \"seed\": 42,\n",
        "    \"sample_train\": 280_000,         # số mẫu train rút ra\n",
        "    \"sample_val\":   20_000,          # số mẫu val rút ra\n",
        "    \"max_length\": 160,               # chiều dài token\n",
        "    \"vocab_size\": 32000,             # kích thước vocab WordPiece\n",
        "    \"min_freq\": 2,                   # tần suất tối thiểu để vào vocab\n",
        "    \"batch_size\": 256,               # sẽ auto giảm nếu thiếu VRAM\n",
        "    \"epochs\": 6,\n",
        "    \"lr\": 5e-4,                      # AdamW\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"d_model\": 256,                  # kích thước embedding\n",
        "    \"n_heads\": 8,\n",
        "    \"n_layers\": 4,\n",
        "    \"ffn_mult\": 4,\n",
        "    \"dropout\": 0.1,\n",
        "    \"save_dir\": \"/content/voz_hsd_transformer\",\n",
        "    \"train_file\": \"/content/voz_hsd_train.jsonl\",\n",
        "    \"val_file\": \"/content/voz_hsd_val.jsonl\",\n",
        "    \"tokenizer_file\": \"/content/voz_hsd_tokenizer.json\",\n",
        "}\n",
        "os.makedirs(CONFIG[\"save_dir\"], exist_ok=True)\n",
        "random.seed(CONFIG[\"seed\"])\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n"
      ],
      "metadata": {
        "id": "HDFBuW0aXD8p"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "def stream_sample_to_files(hf_path, n_train, n_val):\n",
        "    total_needed = n_train + n_val\n",
        "    ds_stream = load_dataset(hf_path, split=\"train\", streaming=True)\n",
        "    # Lọc mẫu hợp lệ + ghi file\n",
        "    train_f = open(CONFIG[\"train_file\"], \"w\", encoding=\"utf-8\")\n",
        "    val_f   = open(CONFIG[\"val_file\"], \"w\", encoding=\"utf-8\")\n",
        "    cnt = 0\n",
        "    val_take_every = max(1, math.floor(n_train / n_val))  # ví dụ 14:1\n",
        "    for ex in ds_stream:\n",
        "        txt = ex.get(\"texts\", None)\n",
        "        lab = ex.get(\"labels\", None)\n",
        "        if txt is None or lab is None:\n",
        "            continue\n",
        "        if not isinstance(lab, int):\n",
        "            try:\n",
        "                lab = int(lab)\n",
        "            except:\n",
        "                continue\n",
        "        record = {\"text\": txt, \"label\": int(lab)}\n",
        "        if (cnt % (val_take_every + 1)) == 0 and n_val > 0:\n",
        "            json.dump(record, val_f, ensure_ascii=False); val_f.write(\"\\n\"); n_val -= 1\n",
        "        else:\n",
        "            json.dump(record, train_f, ensure_ascii=False); train_f.write(\"\\n\"); n_train -= 1\n",
        "        cnt += 1\n",
        "        if n_train <= 0 and n_val <= 0:\n",
        "            break\n",
        "    train_f.close(); val_f.close()\n",
        "    print(f\"Done sampling: wrote {CONFIG['train_file']} and {CONFIG['val_file']}\")\n",
        "\n",
        "# Chạy nếu file chưa tồn tại\n",
        "if not (os.path.exists(CONFIG[\"train_file\"]) and os.path.exists(CONFIG[\"val_file\"])):\n",
        "    stream_sample_to_files(CONFIG[\"hf_path\"], CONFIG[\"sample_train\"], CONFIG[\"sample_val\"])\n",
        "else:\n",
        "    print(\"Sample files already exist, skip sampling.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "e5904d3f85fd427897f038e1bf718feb",
            "2755e2a323c3412da299098c52db702f",
            "ad27065988314aa4ba2059fdc670d003",
            "ead5cfc8f5514484a88e7f8f23859908",
            "dff4b07753f54db39701bc8ec1af9bd1",
            "8ab7664ef5f3427bb8ab2af9b48c3cb9",
            "2ede1563eb6d4c2da2f37a689fcde905",
            "78dcb198a6964496b83992b262e0b34f",
            "4d54f9becc454f37bb4e063445ab636f",
            "5933ac52ad0f4a68a79d4b186e80f902",
            "7d80ba12b5b24864a2e3d64cd5dfcfb2"
          ]
        },
        "id": "7kOjcK8iXFL3",
        "outputId": "485eee8f-2f74-4d97-b2e5-9338577c49f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5904d3f85fd427897f038e1bf718feb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done sampling: wrote /content/voz_hsd_train.jsonl and /content/voz_hsd_val.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(jsonl_path, save_path, vocab_size=32000, min_freq=2):\n",
        "    # Iterator đọc text\n",
        "    def text_iter():\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    obj = json.loads(line)\n",
        "                    yield obj[\"text\"]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "    tokenizer.normalizer = Sequence([NFKC(), Lowercase()])\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = WordPieceTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=min_freq,\n",
        "        special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "    )\n",
        "    tokenizer.train_from_iterator(text_iter(), trainer=trainer, length=CONFIG[\"sample_train\"])\n",
        "    tokenizer.post_processor = TemplateProcessing(\n",
        "        single=\"[CLS] $A [SEP]\",\n",
        "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "        special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\"))],\n",
        "    )\n",
        "    tokenizer.enable_truncation(max_length=CONFIG[\"max_length\"])\n",
        "    tokenizer.enable_padding(length=CONFIG[\"max_length\"], pad_token=\"[PAD]\")\n",
        "    tokenizer.save(save_path)\n",
        "    print(\"Saved tokenizer to\", save_path)\n",
        "\n",
        "if not os.path.exists(CONFIG[\"tokenizer_file\"]):\n",
        "    train_tokenizer(CONFIG[\"train_file\"], CONFIG[\"tokenizer_file\"], CONFIG[\"vocab_size\"], CONFIG[\"min_freq\"])\n",
        "else:\n",
        "    print(\"Tokenizer already exists:\", CONFIG[\"tokenizer_file\"])\n",
        "\n",
        "tokenizer = Tokenizer.from_file(CONFIG[\"tokenizer_file\"])\n",
        "PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n",
        "CLS_ID = tokenizer.token_to_id(\"[CLS]\")\n",
        "print(\"Vocab size:\", tokenizer.get_vocab_size(), \"PAD:\", PAD_ID, \"CLS:\", CLS_ID)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEgIxkiVXGqU",
        "outputId": "c60a1ab4-fe60-4691-8906-4bd11be8a513"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tokenizer to /content/voz_hsd_tokenizer.json\n",
            "Vocab size: 32000 PAD: 0 CLS: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JsonlTextDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, max_length):\n",
        "        self.path = path\n",
        "        self.samples = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                self.samples.append((obj[\"text\"], int(obj[\"label\"])))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.samples[idx]\n",
        "        enc = self.tokenizer.encode(text)\n",
        "        ids = enc.ids\n",
        "        attn = [0 if i==PAD_ID else 1 for i in ids]\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(attn, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = JsonlTextDataset(CONFIG[\"train_file\"], tokenizer, CONFIG[\"max_length\"])\n",
        "val_ds   = JsonlTextDataset(CONFIG[\"val_file\"], tokenizer, CONFIG[\"max_length\"])\n",
        "\n",
        "def collate(batch):\n",
        "    ids = torch.stack([b[0] for b in batch], dim=0)\n",
        "    attn = torch.stack([b[1] for b in batch], dim=0)\n",
        "    y    = torch.stack([b[2] for b in batch], dim=0)\n",
        "    return ids, attn, y\n",
        "\n",
        "def make_loader(ds, batch_size, shuffle):\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True, collate_fn=collate)\n",
        "\n",
        "BATCH = CONFIG[\"batch_size\"]\n",
        "train_loader = make_loader(train_ds, BATCH, True)\n",
        "val_loader   = make_loader(val_ds,   BATCH, False)\n",
        "len(train_ds), len(val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_4KCHL5XH6I",
        "outputId": "03f7995f-fed0-4730-d4b1-1b1857efda34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(280000, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=2048):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, D)\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:, :L, :]\n",
        "\n",
        "class TransformerEncoderClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=4, ffn_mult=4, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.pad_id = pad_id\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*ffn_mult,\n",
        "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, ids, attn_mask):\n",
        "        x = self.tok_emb(ids)\n",
        "        x = self.pos(x)\n",
        "        # key padding mask: True for PAD positions\n",
        "        key_pad = (ids == self.pad_id)\n",
        "        x = self.encoder(x, src_key_padding_mask=key_pad)\n",
        "        # take CLS token embedding (position 0)\n",
        "        cls = x[:, 0, :]\n",
        "        cls = self.norm(cls)\n",
        "        logits = self.classifier(cls)\n",
        "        return logits\n",
        "\n",
        "model = TransformerEncoderClassifier(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    d_model=CONFIG[\"d_model\"],\n",
        "    n_heads=CONFIG[\"n_heads\"],\n",
        "    n_layers=CONFIG[\"n_layers\"],\n",
        "    ffn_mult=CONFIG[\"ffn_mult\"],\n",
        "    dropout=CONFIG[\"dropout\"],\n",
        "    pad_id=PAD_ID\n",
        ").to(device)\n",
        "\n",
        "sum(p.numel() for p in model.parameters())/1e6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEok71YiXI_-",
        "outputId": "dd6549e1-3870-4030-b577-1936701e50e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.417858"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "def build_cosine_schedule(optimizer, num_warmup, num_train_steps):\n",
        "    def lr_lambda(step):\n",
        "        if step < num_warmup:\n",
        "            return float(step) / float(max(1, num_warmup))\n",
        "        progress = float(step - num_warmup) / float(max(1, num_train_steps - num_warmup))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, golds = [], []\n",
        "    with torch.no_grad():\n",
        "        for ids, attn, y in loader:\n",
        "            ids, attn, y = ids.to(device), attn.to(device), y.to(device)\n",
        "            logits = model(ids, attn)\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            preds.extend(pred.tolist())\n",
        "            golds.extend(y.tolist())\n",
        "    acc = accuracy_score(golds, preds)\n",
        "    f1  = f1_score(golds, preds)\n",
        "    return acc, f1\n",
        "\n",
        "train_steps = len(train_loader) * CONFIG[\"epochs\"]\n",
        "warmup_steps = int(CONFIG[\"warmup_ratio\"] * train_steps)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
        "scheduler = build_cosine_schedule(optimizer, warmup_steps, train_steps)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "best_f1 = 0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GYm-skmXKFP",
        "outputId": "3ce7cdb7-e867-4ad0-84f2-739549735c55"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3166512035.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(True)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad_(True)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1, CONFIG[\"epochs\"]+1):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{CONFIG['epochs']}\")\n",
        "    running = 0.0\n",
        "    for ids, attn, y in pbar:\n",
        "        ids, attn, y = ids.to(device), attn.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(ids, attn)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        # gradient clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"grad_clip\"])\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        running += loss.item()\n",
        "        pbar.set_postfix(loss=f\"{running / (pbar.n or 1):.4f}\")\n",
        "    # eval\n",
        "    acc, f1 = evaluate(model, val_loader)\n",
        "    print(f\"[Val] epoch {epoch}: acc={acc:.4f} | f1={f1:.4f}\")\n",
        "    # save best\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save({\n",
        "            \"config\": CONFIG,\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"vocab_size\": tokenizer.get_vocab_size(),\n",
        "            \"pad_id\": PAD_ID,\n",
        "        }, os.path.join(CONFIG[\"save_dir\"], \"best_model.pt\"))\n",
        "        # lưu tokenizer\n",
        "        os.system(f\"cp {CONFIG['tokenizer_file']} {CONFIG['save_dir']}/tokenizer.json\")\n",
        "        print(\"✅ Saved best to\", CONFIG[\"save_dir\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVP7qgs7XOi1",
        "outputId": "ecd783e5-1112-40db-b6db-61b2ac0eeb52"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/6:   0%|          | 0/1094 [00:00<?, ?it/s]/tmp/ipython-input-2313008464.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 1/6: 100%|██████████| 1094/1094 [01:57<00:00,  9.30it/s, loss=0.0744]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Val] epoch 1: acc=0.9667 | f1=0.6457\n",
            "✅ Saved best to /content/voz_hsd_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2/6:   0%|          | 0/1094 [00:00<?, ?it/s]/tmp/ipython-input-2313008464.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 2/6: 100%|██████████| 1094/1094 [01:57<00:00,  9.31it/s, loss=0.0693]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Val] epoch 2: acc=0.9674 | f1=0.6646\n",
            "✅ Saved best to /content/voz_hsd_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3/6:   0%|          | 0/1094 [00:00<?, ?it/s]/tmp/ipython-input-2313008464.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 3/6: 100%|██████████| 1094/1094 [01:57<00:00,  9.31it/s, loss=0.0572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Val] epoch 3: acc=0.9629 | f1=0.6791\n",
            "✅ Saved best to /content/voz_hsd_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4/6:   0%|          | 0/1094 [00:00<?, ?it/s]/tmp/ipython-input-2313008464.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 4/6: 100%|██████████| 1094/1094 [01:57<00:00,  9.30it/s, loss=0.0434]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Val] epoch 4: acc=0.9655 | f1=0.6864\n",
            "✅ Saved best to /content/voz_hsd_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5/6:   0%|          | 0/1094 [00:00<?, ?it/s]/tmp/ipython-input-2313008464.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 5/6: 100%|██████████| 1094/1094 [01:57<00:00,  9.30it/s, loss=0.0286]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Val] epoch 5: acc=0.9677 | f1=0.6897\n",
            "✅ Saved best to /content/voz_hsd_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6/6:   0%|          | 0/1094 [00:00<?, ?it/s]/tmp/ipython-input-2313008464.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 6/6: 100%|██████████| 1094/1094 [01:57<00:00,  9.31it/s, loss=0.0198]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Val] epoch 6: acc=0.9662 | f1=0.6826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load best\n",
        "ckpt = torch.load(os.path.join(CONFIG[\"save_dir\"], \"best_model.pt\"), map_location=device)\n",
        "model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
        "\n",
        "def predict(texts: List[str]):\n",
        "    batch = []\n",
        "    for t in texts:\n",
        "        enc = tokenizer.encode(t)\n",
        "        ids = torch.tensor(enc.ids, dtype=torch.long)\n",
        "        attn= torch.tensor([0 if i==PAD_ID else 1 for i in enc.ids], dtype=torch.long)\n",
        "        batch.append((ids, attn))\n",
        "    ids = torch.stack([b[0] for b in batch]).to(device)\n",
        "    attn= torch.stack([b[1] for b in batch]).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(ids, attn)\n",
        "        prob = logits.softmax(-1).tolist()\n",
        "        pred = logits.argmax(-1).tolist()\n",
        "    return pred, prob\n",
        "\n",
        "sample_texts = [\n",
        "    \"Thứ này đọc chán thật.\",\n",
        "    \"Súc vật, ngu loz\",\n",
        "    \"Một con Bò bị tông\",\n",
        "]\n",
        "pred, prob = predict(sample_texts)\n",
        "list(zip(sample_texts, pred, prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31Zs-2m6XQGN",
        "outputId": "fe873b62-dd26-4b9b-80bb-a082a3b4d28b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Thứ này đọc chán thật.', 0, [0.9998008608818054, 0.00019912670541089028]),\n",
              " ('Súc vật, ngu loz', 1, [0.0017155666137114167, 0.9982843995094299]),\n",
              " ('Một con Bò bị tông', 0, [0.9920392036437988, 0.007960781455039978])]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 10 — ONNX export via custom encoder (no unsupported ops) =====\n",
        "import os, math, torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# ---- 1) Định nghĩa encoder thuần MatMul/Softmax (ONNX-friendly) ----\n",
        "class ExportableEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # QKV + Out proj\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        # FFN\n",
        "        self.lin1 = nn.Linear(d_model, d_ff, bias=True)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model, bias=True)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def _split_heads(self, x):   # (B, L, D) -> (B, H, L, d_k)\n",
        "        B, L, D = x.shape\n",
        "        x = x.view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def _merge_heads(self, x):   # (B, H, L, d_k) -> (B, L, D)\n",
        "        B, H, L, d_k = x.shape\n",
        "        x = x.transpose(1, 2).contiguous().view(B, L, H * d_k)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, attention_mask):  # x: (B,L,D), attention_mask: (B,L) with 1 for valid, 0 for pad\n",
        "        # --- Self-Attention ---\n",
        "        q = self.q_proj(x)  # (B,L,D)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        q = self._split_heads(q)  # (B,H,L,d_k)\n",
        "        k = self._split_heads(k)\n",
        "        v = self._split_heads(v)\n",
        "\n",
        "        # scaled dot-product\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (B,H,L,L)\n",
        "\n",
        "        # additive mask: convert 1/0 to 0/-1e9 then unsqueeze for heads\n",
        "        if attention_mask is not None:\n",
        "            # attention_mask: (B,L), 1 valid, 0 pad\n",
        "            # mask for keys: where pad -> -1e9\n",
        "            key_mask = (1.0 - attention_mask.float()) * -1e9  # (B,L)\n",
        "            scores = scores + key_mask.unsqueeze(1).unsqueeze(2)  # broadcast to (B,H,L,L)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)  # (B,H,L,L)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, v)       # (B,H,L,d_k)\n",
        "        context = self._merge_heads(context)  # (B,L,D)\n",
        "        out = self.o_proj(context)            # (B,L,D)\n",
        "\n",
        "        # residual + norm\n",
        "        x = self.ln1(x + self.dropout(out))\n",
        "\n",
        "        # --- FFN ---\n",
        "        f = self.lin2(self.dropout(self.act(self.lin1(x))))\n",
        "        x = self.ln2(x + self.dropout(f))\n",
        "        return x\n",
        "\n",
        "class ExportableEncoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, n_layers, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ExportableEncoderLayer(d_model, n_heads, d_ff, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        return x\n",
        "\n",
        "class ExportableClassifier(nn.Module):\n",
        "    def __init__(self, base, pad_id):\n",
        "        super().__init__()\n",
        "        # lấy cấu trúc & tham số từ model đã huấn luyện\n",
        "        self.pad_id = pad_id\n",
        "        self.tok_emb = base.tok_emb\n",
        "        self.pos     = base.pos\n",
        "        self.norm    = base.norm\n",
        "        self.classifier = base.classifier\n",
        "\n",
        "        # build encoder ONNX-friendly có cùng siêu tham số\n",
        "        d_model = base.encoder.layers[0].linear1.in_features  # = CONFIG[\"d_model\"]\n",
        "        n_heads = base.encoder.layers[0].self_attn.num_heads\n",
        "        d_ff    = base.encoder.layers[0].linear1.out_features\n",
        "        n_layers= len(base.encoder.layers)\n",
        "        self.encoder = ExportableEncoder(d_model, n_heads, d_ff, n_layers, dropout=0.0)  # dropout=0 khi export\n",
        "\n",
        "        # copy trọng số từ nn.TransformerEncoderLayer sang ExportableEncoderLayer\n",
        "        for i, (src, dst) in enumerate(zip(base.encoder.layers, self.encoder.layers)):\n",
        "            # self-attn weights (MHA): in_proj_weight/bias = [Q;K;V]\n",
        "            Wq, Wk, Wv = torch.chunk(src.self_attn.in_proj_weight.detach().clone(), 3, dim=0)\n",
        "            bq, bk, bv = torch.chunk(src.self_attn.in_proj_bias.detach().clone(),   3, dim=0)\n",
        "            with torch.no_grad():\n",
        "                dst.q_proj.weight.copy_(Wq)\n",
        "                dst.k_proj.weight.copy_(Wk)\n",
        "                dst.v_proj.weight.copy_(Wv)\n",
        "                dst.q_proj.bias.copy_(bq)\n",
        "                dst.k_proj.bias.copy_(bk)\n",
        "                dst.v_proj.bias.copy_(bv)\n",
        "\n",
        "                dst.o_proj.weight.copy_(src.self_attn.out_proj.weight.detach())\n",
        "                dst.o_proj.bias.copy_(src.self_attn.out_proj.bias.detach())\n",
        "\n",
        "                # FFN\n",
        "                dst.lin1.weight.copy_(src.linear1.weight.detach())\n",
        "                dst.lin1.bias.copy_(src.linear1.bias.detach())\n",
        "                dst.lin2.weight.copy_(src.linear2.weight.detach())\n",
        "                dst.lin2.bias.copy_(src.linear2.bias.detach())\n",
        "\n",
        "                # LayerNorms\n",
        "                dst.ln1.weight.copy_(src.norm1.weight.detach())\n",
        "                dst.ln1.bias.copy_(src.norm1.bias.detach())\n",
        "                dst.ln2.weight.copy_(src.norm2.weight.detach())\n",
        "                dst.ln2.bias.copy_(src.norm2.bias.detach())\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # attention_mask (B,L) 1=valid, 0=pad (đã xây trong dataloader)\n",
        "        x = self.tok_emb(input_ids)       # (B,L,D)\n",
        "        x = self.pos(x)                   # add PE\n",
        "        x = self.encoder(x, attention_mask.float())  # (B,L,D)\n",
        "        cls = x[:, 0, :]\n",
        "        cls = self.norm(cls)\n",
        "        logits = self.classifier(cls)\n",
        "        return logits\n",
        "\n",
        "# ---- 2) Khởi tạo exportable model & xuất ONNX ----\n",
        "exportable = ExportableClassifier(model, pad_id=PAD_ID).to(device).eval()\n",
        "\n",
        "# Input mẫu ổn định\n",
        "example_text = \"ví dụ chạy thử để export\"\n",
        "enc = tokenizer.encode(example_text)\n",
        "ids  = torch.tensor([enc.ids], dtype=torch.long).to(device)\n",
        "attn = torch.tensor([[0 if i == PAD_ID else 1 for i in enc.ids]], dtype=torch.long).to(device)\n",
        "\n",
        "onnx_path = os.path.join(CONFIG[\"save_dir\"], \"model.onnx\")\n",
        "torch.onnx.export(\n",
        "    exportable,\n",
        "    (ids, attn),\n",
        "    onnx_path,\n",
        "    input_names=[\"input_ids\", \"attention_mask\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\"input_ids\": {0: \"batch\", 1: \"seq\"},\n",
        "                  \"attention_mask\": {0: \"batch\", 1: \"seq\"},\n",
        "                  \"logits\": {0: \"batch\"}},\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True,\n",
        ")\n",
        "print(\"✅ Saved ONNX:\", onnx_path)\n",
        "\n",
        "# ---- 3) Kiểm tra nhanh bằng ONNXRuntime (optional) ----\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    import numpy as np\n",
        "    sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "    logits = sess.run(\n",
        "        [\"logits\"],\n",
        "        {\n",
        "            \"input_ids\": ids.detach().cpu().numpy().astype(\"int64\"),\n",
        "            \"attention_mask\": attn.detach().cpu().numpy().astype(\"int64\"),\n",
        "        }\n",
        "    )[0]\n",
        "    print(\"ONNXRuntime test ok. logits shape:\", logits.shape)\n",
        "except Exception as e:\n",
        "    print(\"[WARN] ONNXRuntime quick test skipped/failed:\", repr(e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neniTC3NXRQg",
        "outputId": "9b3f19b9-005f-4709-b8f8-3aea7a1c1900"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2788693275.py:153: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved ONNX: /content/voz_hsd_transformer/model.onnx\n",
            "ONNXRuntime test ok. logits shape: (1, 2)\n"
          ]
        }
      ]
    }
  ]
}